import torch
import clip
from openai import OpenAI
from PIL import Image
from sentence_transformers import SentenceTransformer, util
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv('GPT_API_KEY'))

def get_question_prompt(description: str):
    return f"Generate a quiz question based on this image description: {description}"

def get_gpt_agent(prompt: str):
    return  [{"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"Create a multiple-choice quiz question based on this description: {prompt}\nProvide four answer options."}]



class GPT_ImageQuizWrapper:
    def __init__(self, gpt_model, image_processor):
        """
        Initialize wrapper with GPT and image processor.

        Args:
            gpt_model: The GPT model to generate questions and process answers.
            image_processor: The image processor to analyze images and extract descriptions.
        """
        self.gpt_model = gpt_model
        self.image_processor = image_processor
        self.images = []
        self.image_descriptions = []
        self.quiz_questions = []
        self.user_answers = []

    def add_images(self, images):
        """
        Takes list of images and processes to generate descriptions.

        Args:
            images (list): List of image paths.
        """
        for image in images:
            description = self.image_processor.analyze_image(image)
            if description:
                self.images.append(image)
                self.image_descriptions.append(description)
            else:
                print(f"Failed to process image: {image}")

    def generate_quiz(self):
        """
        Generates quiz questions based on image descriptions.

        Returns:
            list: List of quiz questions generated by GPT based on image descriptions.
        """
        self.quiz_questions = []
        for description in self.image_descriptions:
            question_prompt = get_question_prompt(description)
            question = self.gpt_model.generate_question(question_prompt)
            if question:
                self.quiz_questions.append(question)
            else:
                self.quiz_questions.append("Question generation failed.")
        return self.quiz_questions

    def collect_answers(self, answers):
        """
        Collects answers to the quiz questions.

        Args:
            answers (list): List of answers.
        """
        self.user_answers = answers

    def match_images_to_answers(self):
        """
        Matches the user's answers to the original images based on their descriptions.

        Returns:
            list: A list of matched images based on user answers.
        """
        matched_images = []
        for answer in self.user_answers:
            for idx, description in enumerate(self.image_descriptions):
                print(description)
                print(answer)
                print("--------------")
                if self.gpt_model.match_answer_to_description(answer, description):
                    matched_images.append(self.images[idx])
                    break
        return matched_images


class ImageProcessor:
    def __init__(self):
        # Load the CLIP model and preprocess method
        self.model, self.preprocess = clip.load("ViT-B/32", device="cpu")

    def analyze_image(self, image_path):
        """
        Uses CLIP to analyze image and return a description.

        Args:
            image_path (str): The path to the image to be processed.

        Returns:
            str: Description of the image.
        """
        try:
            # Open and preprocess the image
            image = Image.open(image_path).convert("RGB")
            image_input = self.preprocess(image).unsqueeze(0).to("cpu")

            # Predefined set of text prompts (you can add more prompts if needed)
            text_prompts = [
                "a dog", "a cat", "a sunset", "a beach",
                "a mountain", "a cityscape", "a forest",
                "a river", "a car", "a person", "a building",
                "a boat", "a flower", "a tree", "an airplane"
            ]
            text_inputs = clip.tokenize(text_prompts).to("cpu")

            # Run the model and calculate similarity scores
            with torch.no_grad():
                image_features = self.model.encode_image(image_input)
                text_features = self.model.encode_text(text_inputs)

                # Normalize features
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)

                # Compute cosine similarity
                similarities = (image_features @ text_features.T).softmax(dim=-1)
                values, indices = similarities[0].topk(1)

            # Return the description with the highest similarity score
            return f"This image is most likely showing: {text_prompts[indices[0].item()]}"
        except Exception as e:
            print(f"Error processing image {image_path}: {e}")
            return "Description unavailable."


class GPTModel:
    def __init__(self, api_key):
        # Initialize the sentence transformer model for semantic similarity
        self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_question(self, prompt):
        """
        Uses GPT to generate a quiz question based on the prompt.

        Args:
            prompt (str): The text description or prompt to generate the question from.

        Returns:
            str: Generated quiz question.
        """
        try:
            response = client.chat.completions.create(model="gpt-4-turbo",
            messages=get_gpt_agent(prompt),
            max_tokens=100)
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error generating question: {e}")
            return "Question generation failed."

    def match_answer_to_description(self, answer, description, threshold=0.7):
        """
        Uses semantic similarity to match user's answer to image description.

        Args:
            answer (str): The user's answer.
            description (str): The description of the image.
            threshold (float): Similarity score threshold to determine a match.

        Returns:
            bool: True if the answer is semantically similar to the description, False otherwise.
        """
        try:
            # Compute embeddings
            answer_embedding = self.similarity_model.encode(answer, convert_to_tensor=True)
            description_embedding = self.similarity_model.encode(description, convert_to_tensor=True)

            # Compute cosine similarity
            cosine_score = util.pytorch_cos_sim(answer_embedding, description_embedding).item()

            # Return True if similarity score is above the threshold
            return cosine_score >= threshold
        except Exception as e:
            print(f"Error matching answer to description: {e}")
            return False


if __name__ == "__main__":
    # Initialize the ImageProcessor and GPTModel
    image_processor = ImageProcessor()
    gpt_model = GPTModel(api_key=os.getenv('GPT_API_KEY'))

    # Initialize the GPT_ImageQuizWrapper
    quiz_wrapper = GPT_ImageQuizWrapper(gpt_model=gpt_model, image_processor=image_processor)

    # Add some image paths
    image_paths = ["./images/image1.jpeg", "./images/image2.jpeg", "./images/image3.jpeg"]
    quiz_wrapper.add_images(image_paths)

    # Generate quiz questions
    questions = quiz_wrapper.generate_quiz()
    print("Quiz Questions:")
    for idx, question in enumerate(questions, 1):
        print(f"{idx}. {question}")

    # Simulate user answers
    user_answers = [
        "A beautiful sunset over the ocean.",
        "A playful dog running on the beach.",
        "A city skyline with tall buildings."
    ]
    quiz_wrapper.collect_answers(user_answers)

    # Match answers to images
    matched_images = quiz_wrapper.match_images_to_answers()
    print("\nMatched Images:")
    for image in matched_images:
        print(image)
